[Unit]
Description=vllm docker container
Requires=docker.service
After=docker.service

[Service]
ExecStartPre=/usr/bin/docker pull vllm/vllm-openai:latest
ExecStart=docker run --rm --gpus all --name %n -v /mnt/model:/model -p 8000:8000 --ipc=host vllm/vllm-openai:latest --model /model --served-model-name $MODEL_NAME --enable-prompt-tokens-details
ExecStop=-/usr/bin/docker stop %n

TimeoutStartSec=2h
StandardOutput=journal+console
StandardError=journal+console

# Resource accounting (for health-monitor)
CPUAccounting=yes
MemoryAccounting=yes
IOAccounting=yes
